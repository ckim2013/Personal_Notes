# Sets

## Set and Integer Set
- Concept of a set (abstract data type): Inside of a set are objects and each object can only exist at most once in a set. A set is also not ordered (ex. there is no first element in a set) and can have different types of objects in one set.
- A set can have an API `include`, `insert`, and `delete`.
- As a naiive newbie, I used to implement a set using an array.
  - Ex. `[2, 1, 'hi', 0]`. You would wrap this array in a set class and not allow anyone to index into this array because there is no such thing as things like a second element in a set. All we can do is pushing something into the array if the element does not exist, delete an element from the set, and check for inclusion by scanning the array until we find what we are looking for.
  - For inclusion, we look at each element. Thus, this takes a length time of `O(n)`.
  - For insertion, we also look at each element of the array to see if a duplicate exists. If there is a duplicate, we do nothing but if it does not exist already, we push the element into the end. This is also `O(n)` because of this scan.
  - For deletion, we again look at each element of the array to delete the particular element. This is also `O(n)`. If we can't find the element to delete, that should result in an no-op.
  - All this `O(n)` is bad because it is slow! But it is a good starting point.
- Here is a better way to implement a set. We are going to restrict the set of all possible values that can be inserted into our set. We can do this by saying that we cannot put in values of different types, only same types (like integers). For example, we would have `{2, 1, 4, 0}`.
  - We are going to look at the range between the smallest and the largest element. Smallest is `0` and the largest is `4` which we wouldn't know first. We have to assume that we know all of the elements in the set beforehand.
  - We are going to store an array that has exactly as many indices as the range of my potential elements in my set.
    - We initialize our set as `[false, false, false, false, false]` (indices from 0 to 4).
    - If `0` exists in my set, I will put `true` in the first element of the blank array, and so on. If something does not exist, I will leave as `false`. Now we have a set that maps on to the integers: `[true, true, true, false, true]` (there is a false because `3` does not exist). We can query this set by taking the index of the element we are looking for. If we wanted to see if `2` exists in our set, we would do `[true, true, true, false, true][2]`.
    - If we wanted to add something like `2` to the set, we would set the value at `2` to be `true`.
    - If we wanted to delete something like `2` from the set, we would set the value at `2` to be `false`.
    - This is efficient because indexing into an array takes `O(1)` time (constant time) regardless of how large the index is.
      - An array is contiguous stored data. Stored inside contiguous blocks in RAM, are data (binary encoding of the elements in an array) with ram addresses. These are stored right next to each other just like how certain elements are next to each other in an array. The array object knows the ram address and the length. If we know the initial ram address and the index we want to look up, all we have to do is take the `ram address + 8 * index`. With an index, we would find a new ram address which will point to the element in that index. This is also known as pointer arithmetic which utilizes a simple idea that all of the data are stored right next to each other in RAM in sequential order with no gaps. This is why it is `0(1)` time, and this is why array indexing is so fast.
  - Despite having `O(1)` for `insert`, `include`, and `delete` from pointer arithmetic, this is **still bad!!!** This is not applicable to other things like floats but mainly, this takes up abysmal space. For example, lets have a set with 2 elements `{0; 10^20}`. This will take up way too much RAM. Space complexity is how big the space grows that your algorithm requires as the input gets better. The space complexity for this would be `O(range)` or `O(max - min)`. Because of this, we have to **abandon all of this!!!!**

  ## Resizing Set
  - Let's create a data structure using the set with element `{1, 4, 6, 64, 128, 129}`. Let's fix this to size four. The data structure will be `[__, __, __, __]`, with each space being a bucket. We will take the elements and find some way to determine which bucket it belongs to. That way, we can query the bucket that has the element we are looking for. We can use the modulo `%` for this. Any number mod another number, will give you a range from 0 up to that number - 1 or `(0...n)`. If we take each value and mod them by the number of buckets, that will place the elements into a bucket at index i where i will equal to the result of the mod. So if wanted to search for `6`, `6 % 4` is 2. So `6` will be in the bucket at index 2 which we will then scan for `6`. If we find `6`, we know it is in the set and if we don't find `6`, it is not in the set and it will return `false`.
  - For insertion, we add an element, mod it by four, find the index which equals to the mod result, and insert the element into that bucket in that index. For deleting an element, we do the same mod process, look through the bucket, delete the element if it is in that bucket or return no-up if the element does not exist. This is great for space complexity!
  - For `include?`, if we have a `k` fixed number of buckets (constant), there will be `n/k` many elements (average per bucket) where `n` is the amount of elements. This reduces to the time complexity of `O(n)` from `O(n/k)`. For `insert`, it is also `O(n)` because we need to scan through the bucket to see if the element is already in there. For `remove`, it is the same exact thing, `O(n)`. This is no better than the previous implementation that also has time complexity of `O(n)`. **But** we can turn our `O(n)` to `O(1)` by making the `k` compete with `n`.
  - To increase `k`, we can trigger a resize. We will make a new data structure (array) with five buckets if we want to add a fifth element to our four bucket array from the previous example. The elements in the previous array cannot be copied into the same buckets though since this will mess up the mod calculation when you are searching for an element. So we have to remod everything with `5` and then transfer it over to the new array with five buckets. Inserting into new buckets will require a time complexity of `O(n)` sadly. However, `k` and `n` are now keeping pace with each other. Average number of things in the bucket will be `n/k` but if `k` is equal to n or even greater, we get `n/n` or `1` which means the average number of things in a bucket is 1 which is a **constant!** `include` and `delete` is now `O(1)` since on average, a bucket has only 1, 2, or maybe even 3 elements. Adding more than one bucket when we add an element will not help with the `O(n)` of insertion. However...
  -  Instead of increasing the buckets by a constant amount (when we add elements), we can increase the buckets by a constant multiple whenever we hit a capacity limit. You get free pushes whenever you create new buckets. Amortized cost is constant no matter how large `n` gets. There is no promise that you will get `O(1)` at any particular push but you can promise with certainty that with many pushes it will cost something proportional to `n`. Amortized is not the same as average. Now `insert` (amortized), `include?`, and `delete` are `O(1)` and space complexity is `O(n)` (because the number of buckets scales with `n`). **(NEED TO REVIST)**
  - Given a pathological input will ruin these nice `O(1)`. If we constantly get elements in one bucket (if mod result is 0), then we will now have `O(n)` again.
